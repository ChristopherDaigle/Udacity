{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### Using squared errors instead of log loss\n",
    "\n",
    "We want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the **error**. A common metric is the sum of the squared errors (**SSE**):\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{\\mu} \\sum_{j} [y_{j}^{\\mu}-\\hat{y}_{j}^{\\mu}]^{2} $$\n",
    "\n",
    "where $\\hat{y}$ s the prediction and $y$ is the true value, and you take the sum over all output units $j$ and another sum over all data points $\\mu$.\n",
    "\n",
    "First, the inside sum over $j$. \n",
    "> This variable $j$ represents the output units of the network. So this inside sum is saying \"for each output unit, find the difference between the true value $y$ and the predicted value $\\hat{y}$, the square the difference, then sum all those squares.\"\n",
    "    \n",
    "Then, the other sum over $\\mu$ is a sum over all the data points.\n",
    "> For each data point you calculate the inner sum of the squared differences for each output unit. Then, you sum those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.\n",
    "\n",
    "The **SSE** is a good choice for a few reasons\n",
    "> The square ensures the error is always positive and larger errors are penalized more than smaller errors\n",
    "> it makes the math nice (always a good thing)\n",
    "\n",
    "Output of the neural network depends on weights:\n",
    "$$ \\hat{y}_{j}^{\\mu} = f (\\sum_{i}w_{ij}x_{i}^{\\mu} )$$\n",
    "\n",
    ".. and accordingly, the error depends on the weights:\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{\\mu} \\sum_{j}[ y_{j}^{\\mu}-f(\\sum_{i}w_{ij}x_{i}^{\\mu})]^{2}$$\n",
    "\n",
    "We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights $w_{ij}$ that minimize the squared error $E$. To do this with a neural network, typically you'd use **gradient descent**.\n",
    "\n",
    "**A mountain as a gradient descent analogy:**\n",
    "Since the fastest way down a mountain is in the steepest direction, the steps taken should be in the direction that minimizes the error the most. We can find this direction by calculating the *gradient* of the squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='MultiNNGrad.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One weight update can be calculated as:\n",
    "$$\\Delta w_{i} = \\eta \\delta x_{i} $$\n",
    "\n",
    "with the error term $\\delta$ as\n",
    "$$\\delta = (y-\\hat{y})f^{'}(\\sum w_{i}x_{i}) $$\n",
    "\n",
    "Remember, in the above equation, $(y-\\hat{y})$ is the output error and $f^{'}(h)$ refers ti the derivative of the activation function, $f(h)$ We'll call that derivative the output gradient.\n",
    "\n",
    "Now, we'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function $f(h)$.\n",
    "\n",
    "```python\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta, in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# The linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# Output error (y - y-hat)\n",
    "error = y- nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# Error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step\n",
    "del_w = [learnrate * error_term * x[0],\n",
    "         learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consolidated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = x[0]*w[0] + x[1]*w[1] + x[2]*w[2] + x[3]*w[3]\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "output_grad = sigmoid_prime(h)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
