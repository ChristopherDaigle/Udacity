{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Classification algorithm that looks to set a boundary with as wide a margin as possible\n",
    "\n",
    "<img src='SVMOverview.png'>\n",
    "\n",
    "$$ Margin = \\frac{2}{|W|}$$\n",
    "$$ Error = |W|^{2} $$\n",
    "Example:\n",
    "\n",
    "$$ W = (3,4), b=1;$$\n",
    "$$w_{1}x_{1}+w_{2}x_{2}+b=0 \\rightarrow 3x_{1}+4x_{2}+1 $$\n",
    "\n",
    "$$ Error = |W|^{2} = 3^{2}+4^{2} = 25 $$\n",
    "\n",
    "$$Margin = \\frac{2}{|W|} = \\frac{2}{\\sqrt{25}} $$\n",
    "\n",
    "<img src='SVMMarginErrorExample.png'>\n",
    "\n",
    "This is the same error term as in L2 regularization\n",
    "\n",
    "Recall:\n",
    "> ## L1 Regularization\n",
    ">Add the absolute value of the coefficients:\n",
    "$$\n",
    "2x_{1}^{3}-2x_{1}^{2}x_{2} - 4x_{2}^{3} + 3x_{1}^{2} + 6x_{1}x_{2} + 4x_{2}^{2} + 5 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "Error = |2|+|-2|+|-4|+|3|+|6|+|4|= 21\n",
    "$$\n",
    "\n",
    "> ## L2 Regularization\n",
    "> Add the squares of the coefficients:\n",
    "$$\n",
    "2x_{1}^{3}-2x_{1}^{2}x_{2} - 4x_{2}^{3} + 3x_{1}^{2} + 6x_{1}x_{2} + 4x_{2}^{2} + 5 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "Error = 2^{2}+(-2)^{2}+(-4)^{2}+3^{2}+6^{2}+4^{2}= 85\n",
    "$$\n",
    "\n",
    "# Kernel Trick\n",
    "\n",
    "Transform the locations of the points with a function to better separate them than linearly alone. In this case, we have points that look like a bullseye and are thus easily seperable with a circle, i.e. $x^{2}+y^{2}$\n",
    "\n",
    "<img src='KernelTrickGrid.png'>\n",
    "\n",
    "# SVMs with SKLearn\n",
    "\n",
    "tools required to build this model.\n",
    "\n",
    "For your support vector machine model, you'll be using scikit-learn's SVC class. This class provides the functions to define and fit the model to your data.\n",
    "\n",
    "```C```: The C parameter.\n",
    "\n",
    "```kernel```: The kernel. The most common ones are 'linear', 'poly', and 'rbf'.\n",
    "\n",
    "```degree```: If the kernel is polynomial, this is the maximum degree of the monomials in the kernel.\n",
    "\n",
    "```gamma```: If the kernel is rbf, this is the gamma parameter.\n",
    "\n",
    "For example, here we define a model with a polynomial kernel of degree 4, and a C parameter of 0.1.\n",
    "\n",
    "```>>> model = SVC(kernel='poly', degree=4, C=0.1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('SVMData.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the model and assign it to the variable model.\n",
    "# Find the right parameters for this model to achieve 100% accuracy on the dataset.\n",
    "model = SVC(kernel='rbf', gamma=28)\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X,y)\n",
    "\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "In this lesson, you learned about Support Vector Machines (or SVMs). SVMs are a popular algorithm used for classification problems. You saw three different ways that SVMs can be implemented:\n",
    "\n",
    "> 1. Maximum Margin Classifier\n",
    "> 2. Classification with Inseparable Classes\n",
    "> 3. Kernel Methods\n",
    "\n",
    "**Maximum Margin Classifier**\n",
    "\n",
    "When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points (called the support vectors). For this reason, we saw that in the picture below, the boundary on the left is better than the one on the right.\n",
    "<img src='Recap1.png'>\n",
    "\n",
    "**Classification with Inseparable Classes**\n",
    "\n",
    "Unfortunately, data in the real world is rarely completely separable as shown in the above images. For this reason, we introduced a new hyper-parameter called **C**. The **C** hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary. The value of **C** ranges between 0 and infinity. When **C** is large, you are forcing your boundary to have fewer errors than when it is a small value.\n",
    "\n",
    "**Note: when C is too large for a particular set of data, you might not get convergence at all because your data cannot be separated with the small number of errors allotted with such a large value of C.**\n",
    "<img src='Recap2.png'>\n",
    "\n",
    "**Kernels**\n",
    "\n",
    "Finally, we looked at what makes SVMs truly powerful, kernels. Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear. Specifically, you saw two types of kernels:\n",
    "\n",
    "<ul>\n",
    "    <li>polynomial</li>\n",
    "    <li>rbf</li>\n",
    "</ul>\n",
    "    \n",
    "By far the most popular kernel is the **rbf** kernel (which stands for radial basis function). The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. This is a density based approach that looks at the closeness of points to one another. This introduces another hyper-parameter **gamma**. When **gamma** is large, the outcome is similar to having a large value of **C**, that is your algorithm will attempt to classify every point correctly. Alternatively, small values of **gamma** will try to cluster in a more general way that will make more mistakes, but may perform better when it sees new data.\n",
    "<img src='Recap3.png'>\n",
    "\n",
    "# Resources:\n",
    "**Support Vector Machines are described in Introduction to Statistical Learning starting on page 337**\n",
    "\n",
    "**The wikipedia page related to SVMs**\n",
    "\n",
    "**The derivation of SVMs from Stanford's CS229 notes**(http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
