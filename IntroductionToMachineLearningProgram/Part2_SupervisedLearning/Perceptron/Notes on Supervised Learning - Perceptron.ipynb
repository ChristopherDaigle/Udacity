{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Algorithm\n",
    "\n",
    "Perceptrons can be used as logical operators, (and, or, xor, nand) by setting weights on the edges of the inputs and biases. The node calculates some equation/function, and we receive some output.\n",
    "\n",
    "Steps:\n",
    "\n",
    ">1) Inputs (factors and bias)\n",
    "\n",
    ">2) Weights on inputs (applied on \"edges\")\n",
    "\n",
    ">3) Linear function node (produces some value from the weights and inputs)\n",
    "\n",
    ">4) Step Function (takes output from Linear Function node and translates it to a 1 or 0)\n",
    "\n",
    ">5) Step Function Output (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                  -2.0                    0          Yes\n",
      "       0          1                  -1.0                    0          Yes\n",
      "       1          0                  -1.0                    0          Yes\n",
      "       1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -2.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OR perceptron is very similar to an AND perceptron. In the image below, the OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron\n",
    "\n",
    "An **AND** perceptron can be converted to an **OR** perceptron by:\n",
    "\n",
    "> 1) Increasing the weights\n",
    "\n",
    "> 2) Decreasing the magnitude of the bias\n",
    "\n",
    "Another example of a perceptron is the NOT perceptron. Here is a practical application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      " Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "       0          0                   0.0                    1          Yes\n",
      "       0          1                  -1.0                    0          Yes\n",
      "       1          0                   0.0                    1          Yes\n",
      "       1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.\n",
    "bias = -0.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Perceptron\n",
    "\n",
    "**XOR** means \"one or the other, not both\"\n",
    "\n",
    "So, it should output \"False\" when both inputs are \"True\" or both inputs are \"False\"\n",
    "\n",
    "<img src=\"xor.png\">\n",
    "\n",
    "# XOR Multi-Layer Perceptron\n",
    "We can set the **NAND** Perceptron with a combination of **AND** and **NOT**\n",
    "<img src=\"xor-quiz2.png\">\n",
    "\n",
    "# Perceptron Trick:\n",
    "\n",
    "When point is misclassified **above** the line, we **subtract** the points (multiplied by the learning rate) from the weights/coefficients on the classifying line/decision boundary.\n",
    "\n",
    "When point is misclassified **below** the line, we **add** the points (multiplied by the learning rate) from the weights/coefficients on the classifying line/decision boundary.\n",
    "\n",
    "# Programming the Perceptron Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        \n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        \n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Equation for multiclass options:\n",
    "$$\n",
    "Entropy = - \\sum_{i}^{n} p_{i} log_{2}(p_{i})\n",
    "$$\n",
    "\n",
    "# Information Gain\n",
    "This is a measure of change in entropy\n",
    "$$\n",
    "InformationGain = Entropy(Parent) - [\\frac{m}{m+n}*Entropy(Child_{1}) + \\frac{n}{m+n}*Entropy(Child_{2})]\n",
    "$$\n",
    "\n",
    "Parent is the initial node, and the children are the following nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11260735516748976"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 & 24 are the number of the Mobugs and Lobugs\n",
    "# tot = 24, sum of mo and lobugs\n",
    "# tot_ent = parent entropy\n",
    "# g17_ent = \n",
    "\n",
    "def two_group_ent(first, tot):                        \n",
    "    return -(first/tot*np.log2(first/tot)+(tot-first)/tot*np.log2((tot-first)/tot))\n",
    "\n",
    "tot_ent = two_group_ent(10, 24)\n",
    "g17_ent = 15/24 * two_group_ent(11,15)+9/24 * two_group_ent(6,9)                  \n",
    "\n",
    "answer = tot_ent - g17_ent\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of Decision Trees\n",
    "> 1) Maximum Depth\n",
    "> > The maximum depth of a decision tree is simply the largest possible length between the root to a leaf. A tree of maximum length k can have at most $2^{k}$ leaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
